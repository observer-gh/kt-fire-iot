# DataLake

Data ingestion and streaming processing service for IoT fire monitoring.

## ğŸ—ï¸ Architecture

### Data Flow

1. **Real-time Data Ingestion**: ì„¼ì„œ ë°ì´í„°ëŠ” Mock Serverë‚˜ ì™¸ë¶€ APIì—ì„œ ìˆ˜ì‹ ë˜ì–´ Redisì— ì €ì¥ë©ë‹ˆë‹¤.
2. **Redis Storage**: ëª¨ë“  ì„¼ì„œ ë°ì´í„°ëŠ” Redisì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥ë˜ì–´ ë¹ ë¥¸ ì ‘ê·¼ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
3. **Periodic Flush**: 1ë¶„ ê°„ê²©ìœ¼ë¡œ Redisì˜ ë°ì´í„°ë¥¼ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì— flushí•©ë‹ˆë‹¤.
4. **Metadata Storage**: flush ì‹œ ë©”íƒ€ë°ì´í„°(ìˆ˜ì§‘ ê¸°ê°„, ìŠ¤í† ë¦¬ì§€ ìœ„ì¹˜, í†µê³„ ì •ë³´ ë“±)ë¥¼ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤.
5. **Event Publishing**: `sensorDataSaved` Kafka ì´ë²¤íŠ¸ëŠ” flush ì‹œì—ë§Œ ë°œí–‰ë©ë‹ˆë‹¤.
6. **Anomaly Detection**: ì´ìƒì¹˜ íƒì§€ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©° ì¦‰ì‹œ `sensorDataAnomalyDetected` ì´ë²¤íŠ¸ê°€ ë°œí–‰ë©ë‹ˆë‹¤.

### Key Components

- **Redis Client**: ì„¼ì„œ ë°ì´í„°ì˜ ì„ì‹œ ì €ì¥ì†Œ ì—­í• 
- **Batch Scheduler**: Redis flushì™€ ë°°ì¹˜ ì—…ë¡œë“œë¥¼ ê´€ë¦¬í•˜ë©° ë©”íƒ€ë°ì´í„° ìƒì„±
- **Storage Service**: ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì— ë°ì´í„°ë¥¼ ì˜êµ¬ ì €ì¥í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥
- **Kafka Publisher**: ì´ë²¤íŠ¸ ë°œí–‰ì„ ë‹´ë‹¹
- **Metadata Management**: ìŠ¤í† ë¦¬ì§€ flush ì •ë³´ë¥¼ ì¶”ì í•˜ê³  ê´€ë¦¬

### Storage Metadata

Redis flush ì‹œë§ˆë‹¤ ë‹¤ìŒê³¼ ê°™ì€ ë©”íƒ€ë°ì´í„°ê°€ PostgreSQLì˜ `storage_metadata` í…Œì´ë¸”ì— ì €ì¥ë©ë‹ˆë‹¤:

- **ê¸°ë³¸ ì •ë³´**: flush ì‹œê°„, ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„, ë ˆì½”ë“œ ìˆ˜
- **ìŠ¤í† ë¦¬ì§€ ì •ë³´**: ì €ì¥ ê²½ë¡œ, ìŠ¤í† ë¦¬ì§€ íƒ€ì…, íŒŒì¼ í¬ê¸°
- **ì²˜ë¦¬ ì •ë³´**: ì²˜ë¦¬ ì‹œê°„, ì„±ê³µ/ì‹¤íŒ¨ ê°œìˆ˜
- **ì¶”ê°€ ì •ë³´**: ë°°ì¹˜ ID, flush íƒ€ì…, ì—ëŸ¬ ë©”ì‹œì§€ ë“±

ì´ë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘ í˜„í™©ì„ ì¶”ì í•˜ê³  ìŠ¤í† ë¦¬ì§€ íš¨ìœ¨ì„±ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ Quick Start

### With Docker Compose (Recommended)

```bash
# Start all services including DataLake
docker-compose up -d

# Start just DataLake and dependencies
docker-compose up -d postgres redis kafka datalake-api datalake-dashboard

# Start DataLake API only
docker-compose up -d datalake-api

# Start DataLake dashboard only
docker-compose up -d datalake-dashboard
```

### Manual Build & Run

```bash
# Build API service
docker build -f Dockerfile.api -t fire-iot-datalake-api .

# Build dashboard service
docker build -f Dockerfile.dashboard -t fire-iot-datalake-dashboard .

# Run API service with dependencies
docker run -d --name datalake-api --network fire-iot-network -p 8084:8080 \
  -e POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/core \
  -e REDIS_URL=redis://redis:6379 \
  -e KAFKA_BROKERS=kafka:29092 \
  -e STORAGE_TYPE=mock \
  -e BATCH_SIZE=100 \
  -e REDIS_FLUSH_INTERVAL_SECONDS=60 \
  -e MOCK_SERVER_URL=http://mock-server:8081 \
  -e MOCK_SERVER_DATA_COUNT=10 \
  -e MOCK_SERVER_DATA_FETCH_INTERVAL=5 \
  -e MOCK_SERVER_STREAM_COUNT=5 \
  -e MOCK_SERVER_BATCH_COUNT=100 \
  fire-iot-datalake-api

# Run dashboard service
docker run -d --name datalake-dashboard --network fire-iot-network -p 8501:8501 \
  -e POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/core \
  fire-iot-datalake-dashboard
```

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Run API service with hot reload
uvicorn app.main:app --reload --host 0.0.0.0 --port 8080

# Run dashboard
streamlit run app/dashboard/main_dashboard.py --server.port=8501 --server.address=0.0.0.0

# Or use the provided script
./start_dashboard.sh
```

## ğŸ“Š APIs

### Core APIs

- `GET /healthz` - Health check with Redis and Mock Server status
- `GET /redis/status` - Redis connection status and info (includes sensor data count)
- `GET /stats` - Service statistics and storage info (with Redis caching)
- `GET /docs` - API documentation (Swagger UI)

### Storage Metadata APIs (New)

- `GET /storage/metadata` - Get storage metadata with filtering and pagination
- `GET /storage/metadata/{metadata_id}` - Get specific storage metadata by ID
- `GET /storage/metadata/stats/summary` - Get summary statistics of storage metadata

### Data Ingestion APIs

#### Mock Server Integration (New)

- `POST /ingest` - **Mock Serverì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ Redisì— ì €ì¥**
- `POST /ingest/stream` - Mock Serverì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ Redisì— ì €ì¥
- `POST /ingest/batch` - Mock Serverì—ì„œ ë°°ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ Redisì— ì €ì¥

#### Redis Management APIs (New)

- `POST /trigger-redis-flush` - Redis ë°ì´í„°ë¥¼ ê°•ì œë¡œ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì— flush

#### Mock Data Scheduler Control APIs (New)

- `GET /scheduler/mock/status` - Mock Data Scheduler ìƒíƒœ í™•ì¸
- `POST /scheduler/mock/start` - Mock Data Scheduler ì‹œì‘
- `POST /scheduler/mock/stop` - Mock Data Scheduler ì¤‘ì§€
- `POST /scheduler/mock/restart` - Mock Data Scheduler ì¬ì‹œì‘
- `POST /scheduler/mock/force-process` - Mock ë°ì´í„° ì²˜ë¦¬ ê°•ì œ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœì™€ ë¬´ê´€)

#### External API Integration (Legacy)

- `POST /ingest/external` - ì™¸ë¶€ APIì—ì„œ ì„¼ì„œ ë°ì´í„° ìˆ˜ì‹ í•˜ì—¬ Redisì— ì €ì¥

### Storage & Batch APIs

- `POST /trigger-batch-upload` - Manually trigger batch upload
- `GET /storage/batches` - Get uploaded batches (MockStorage only)
- `DELETE /storage/batches` - Clear batch tracking (MockStorage only)
- `DELETE /cache` - Clear Redis cache

## ğŸ”„ Mock Server Integration

DataLakeëŠ” ì´ì œ Mock Serverì™€ ìë™ìœ¼ë¡œ ì—°ë™ë˜ì–´ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì™€ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

### Features

- **ìë™ í´ë§**: ì„¤ì •ëœ ê°„ê²©ìœ¼ë¡œ Mock Serverì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ë°›ì•„ì˜¨ ë°ì´í„°ë¥¼ ì¦‰ì‹œ ì²˜ë¦¬í•˜ê³  ì´ìƒì¹˜ íƒì§€
- **ì´ë²¤íŠ¸ ë°œí–‰**: ì´ìƒì¹˜ ë°œê²¬ ì‹œ ControlTowerë¡œ ì´ë²¤íŠ¸ ë°œí–‰
- **ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…**: ì‹¤ì‹œê°„, ìŠ¤íŠ¸ë¦¬ë°, ë°°ì¹˜ ë°ì´í„° ì§€ì›

### Configuration

```bash
# Mock Server ì„¤ì •
MOCK_SERVER_URL=http://localhost:8081          # Mock Server URL
MOCK_SERVER_DATA_COUNT=10                     # ê¸°ë³¸ ë°ì´í„° ê°œìˆ˜
MOCK_SERVER_DATA_FETCH_INTERVAL=5             # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ê°„ê²© (ì´ˆ) - ê¸°ë³¸ê°’: 5ì´ˆ
MOCK_SERVER_STREAM_COUNT=5                    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ê°œìˆ˜
MOCK_SERVER_BATCH_COUNT=100                   # ë°°ì¹˜ ë°ì´í„° ê°œìˆ˜

# Dashboard ì„¤ì •
DASHBOARD_REFRESH_INTERVAL=1                  # ëŒ€ì‹œë³´ë“œ ìë™ ìƒˆë¡œê³ ì¹¨ ê°„ê²© (ì´ˆ) - ê¸°ë³¸ê°’: 1ì´ˆ
```

### Data Flow

1. **Mock Server** â†’ ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„° ìƒì„± (90% ì •ìƒ, 10% ì´ìƒ)
2. **DataLake** â†’ ì£¼ê¸°ì ìœ¼ë¡œ Mock Serverì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
3. **DataLake** â†’ ë°ì´í„° ì²˜ë¦¬ ë° ì´ìƒì¹˜ íƒì§€
4. **DataLake** â†’ ì´ìƒì¹˜ ë°œê²¬ ì‹œ ControlTowerë¡œ ì´ë²¤íŠ¸ ë°œí–‰
5. **DataLake** â†’ ëª¨ë“  ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥

## ğŸ¯ Real-time Dashboard

The DataLake service includes a comprehensive real-time monitoring dashboard built with Streamlit.

### Dashboard Features

- **Real-time Sensor Monitoring**: Live gauge charts for temperature, humidity, smoke density, CO level, and gas level
- **Historical Data Visualization**: Time series charts with configurable time ranges (1 hour, 24 hours, 7 days)
- **Alert Management**: Real-time alert detection and display with severity levels
- **Data Quality Monitoring**: Detection of missing data and quality issues
- **Storage Metadata Monitoring**: Comprehensive tracking of Redis flush operations and storage efficiency

### Storage Metadata Dashboard (New)

- **Summary Metrics**: Total flushes, records processed, average processing time, recent activity
- **Detailed View**: Recent flushes table with filtering and search capabilities
- **Storage Type Breakdown**: Visual charts showing flush distribution by type
- **Performance Tracking**: Processing duration, success rates, and error counts
- **Filtering & Search**: Date range, storage type, and additional info search

### Access Dashboard

```bash
# Via Docker Compose
docker-compose up -d datalake-dashboard
# Then open http://localhost:8501 in your browser

# Via Local Development
streamlit run app/dashboard/main_dashboard.py --server.port=8501

# Via Script
./start_dashboard.sh
```

### Access API

```bash
# Via Docker Compose
docker-compose up -d datalake-api
# Then access http://localhost:8084

# Via Local Development
uvicorn app.main:app --reload --host 0.0.0.0 --port 8080

# Health check
curl http://localhost:8084/healthz
```

### Dashboard Controls

- **Time Range**: Choose between real-time, 1 hour, 24 hours, or 7 days
- **Sensor Selection**: Show/hide specific sensor types
- **Auto Refresh**: Automatic updates every 30 seconds
- **Manual Refresh**: Manual refresh button for immediate updates

### Alert Thresholds

- **Temperature**: HIGH at 40Â°C, CRITICAL at 60Â°C
- **Humidity**: HIGH at 90%, CRITICAL at 95%
- **Smoke Density**: HIGH at 0.500, CRITICAL at 1.000
- **CO Level**: HIGH at 30.000 ppm, CRITICAL at 50.000 ppm
- **Gas Level**: HIGH at 100.000 ppm, CRITICAL at 200.000 ppm

## ğŸ”„ Data Flow

```
External API â†’ DataLake (clean/process) â†’ Database â†’ Redis Cache â†’ Storage â†’ Kafka â†’ Other Services
```

## ğŸ“ˆ Performance & Monitoring

### Redis Metrics

- **Sensor Data Count**: Redisì— ì €ì¥ëœ í˜„ì¬ ì„¼ì„œ ë°ì´í„° ê°œìˆ˜
- **Flush Interval**: ì„¤ì •ëœ Redis flush ê°„ê²© (ê¸°ë³¸ê°’: 60ì´ˆ)
- **Connection Status**: Redis ì—°ê²° ìƒíƒœ ë° ì‘ë‹µ ì‹œê°„

### Storage Metadata Metrics (New)

- **Total Flushes**: ì „ì²´ flush íšŸìˆ˜
- **Records Processed**: ì²˜ë¦¬ëœ ì´ ë ˆì½”ë“œ ìˆ˜
- **Average Processing Time**: í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
- **Storage Type Breakdown**: ìŠ¤í† ë¦¬ì§€ íƒ€ì…ë³„ flush ë¶„í¬
- **Recent Activity**: ìµœê·¼ 24ì‹œê°„ ë‚´ flush í™œë™

### Event Publishing

- **Anomaly Events**: ì´ìƒì¹˜ íƒì§€ ì‹œ ì¦‰ì‹œ ë°œí–‰ (`sensorDataAnomalyDetected`)
- **Data Saved Events**: Redis flush ì‹œì—ë§Œ ë°œí–‰ (`sensorDataSaved`)
- **Batch Events**: ë°°ì¹˜ ì—…ë¡œë“œ ì‹œ ë°œí–‰

### Storage Efficiency

- **Real-time Storage**: Redisë¥¼ í†µí•œ ë¹ ë¥¸ ë°ì´í„° ì ‘ê·¼
- **Batch Persistence**: ì£¼ê¸°ì ì¸ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ flushë¡œ ë°ì´í„° ì˜ì†ì„± ë³´ì¥
- **Memory Management**: flush í›„ Redis ë°ì´í„° ìë™ ì •ë¦¬
- **Metadata Tracking**: ëª¨ë“  flush ì‘ì—…ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥í•˜ì—¬ ì¶”ì  ê°€ëŠ¥

## ğŸ—„ï¸ Redis Caching

The DataLake service now includes Redis caching for improved performance:

- **Stats Caching**: Service statistics are cached for 5 minutes to reduce database queries
- **Health Monitoring**: Redis connection status is included in health checks
- **Cache Management**: Dedicated endpoints for cache status and clearing
- **Performance**: Faster response times for frequently accessed data

### Redis Configuration

- **URL**: Configurable via `REDIS_URL` environment variable
- **Default**: `redis://redis:6379` (Docker Compose)
- **Fallback**: `redis://localhost:6379` (Local development)
- **Connection**: Automatic connection with health monitoring

### Data Processing

- **Validation**: Sensor data validation and bounds checking
- **Cleaning**: Out-of-range values clamped to valid ranges
- **Anomaly Detection**: Automatic threshold-based anomaly detection
- **Database Storage**: Real-time data stored in PostgreSQL
- **Batch Upload**: Configurable batch processing to file storage
- **Event Publishing**: Kafka topics for anomaly detection and data saved events

### Kafka Topics

- `datalake.sensorDataAnomalyDetected` - Anomaly detection events
- `datalake.sensorDataSaved` - Data saved to storage events
- `datalake.sensorData` - Legacy sensor data events (deprecated)

## ğŸ—„ï¸ Storage Types

### MockStorage (Local Testing)

- **Default**: `STORAGE_TYPE=mock`
- **Batch Size**: 100 records (configurable)
- **Storage**: Local filesystem (JSON files)
- **Features**: Batch tracking, statistics, testing endpoints

### StorageService (Production)

- **Set**: `STORAGE_TYPE=production`
- **Batch Size**: 1000 records (configurable)
- **Storage**: Production storage paths
- **Features**: Production-optimized batch processing

## ğŸ§ª Testing

### Test Data Ingestion

```bash
# Normal sensor data
curl -X POST http://localhost:8084/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "equipment_id": "EQ001",
    "facility_id": "FAC001",
    "equipment_location": "Building A, Floor 1",
    "measured_at": "2024-01-01T12:00:00Z",
    "temperature": 25.5,
    "humidity": 60.2,
    "smoke_density": 0.001,
    "co_level": 0.005,
    "gas_level": 0.002
  }'

# Temperature anomaly (above 80Â°C threshold)
curl -X POST http://localhost:8084/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "equipment_id": "EQ002",
    "facility_id": "FAC001",
    "measured_at": "2024-01-01T12:00:00Z",
    "temperature": 85.0,
    "humidity": 65.0
  }'

# Smoke critical anomaly (above 500ppm threshold)
curl -X POST http://localhost:8084/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "equipment_id": "EQ003",
    "facility_id": "FAC001",
    "measured_at": "2024-01-01T12:00:00Z",
    "smoke_density": 600.0
  }'
```

### Batch Processing Test

````bash
# Send 100 records to trigger batch upload
for i in {1..100}; do
  curl -X POST http://localhost:8084/ingest \
    -H "Content-Type: application/json" \
    -d "{
      \"equipment_id\": \"EQ00$i\",
      \"measured_at\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
      \"temperature\": $((20 + i % 10))
    }"
done

# Check batch upload status
curl http://localhost:8084/storage/batches

### Mock Data Scheduler Control

```bash
# Check scheduler status
curl http://localhost:8084/scheduler/mock/status

# Start the scheduler
curl -X POST http://localhost:8084/scheduler/mock/start

# Stop the scheduler
curl -X POST http://localhost:8084/scheduler/mock/stop

# Restart the scheduler
curl -X POST http://localhost:8084/scheduler/mock/restart

# Force process mock data once (even if scheduler is stopped)
curl -X POST http://localhost:8084/scheduler/mock/force-process
curl http://localhost:8084/stats

# Manually trigger batch upload
curl -X POST http://localhost:8084/trigger-batch-upload

# View uploaded batches (MockStorage only)
curl http://localhost:8084/storage/batches
````

### Health Check

```bash
curl http://localhost:8084/healthz
```

## ğŸ“ Structure

```
app/
â”œâ”€â”€ main.py              # FastAPI application + endpoints
â”œâ”€â”€ config.py            # Configuration management
â”œâ”€â”€ database.py          # Database connection
â”œâ”€â”€ db_models.py         # SQLAlchemy ORM models
â”œâ”€â”€ models.py            # Pydantic data models
â”œâ”€â”€ processor.py         # Data cleaning & anomaly detection
â”œâ”€â”€ publisher.py         # Kafka event publishing
â”œâ”€â”€ scheduler.py         # Batch upload scheduler
â”œâ”€â”€ storage_interface.py # Storage interface abstraction
â”œâ”€â”€ storage_service.py   # Production storage service
â””â”€â”€ mock_storage.py      # Mock storage for testing
```

## ğŸ”§ Configuration

### Environment Variables

- `POSTGRES_URL` - PostgreSQL connection string
- `POSTGRES_USER` - PostgreSQL username
- `POSTGRES_PASSWORD` - PostgreSQL password
- `POSTGRES_HOST` - PostgreSQL host
- `POSTGRES_PORT` - PostgreSQL port
- `POSTGRES_DB` - PostgreSQL database name
- `REDIS_URL` - Redis connection string
- `KAFKA_BROKERS` - Kafka broker addresses
- `STORAGE_TYPE` - Storage type: `mock` or `production`
- `BATCH_SIZE` - Batch size for uploads
- `BATCH_INTERVAL_MINUTES` - Batch processing interval
- `STORAGE_PATH` - Storage directory path

### Anomaly Detection Thresholds

- **Temperature**: Alert at 80Â°C (WARN), 95Â°C (EMERGENCY)
- **Humidity**: Alert at 90% (WARN), 98% (EMERGENCY)
- **Smoke Density**: Alert at 100ppm (WARN), 500ppm (EMERGENCY)
- **CO Level**: Alert at 50ppm (WARN), 200ppm (EMERGENCY)
- **Gas Level**: Alert at 100ppm (WARN), 500ppm (EMERGENCY)

### Database Schema

The service uses the following tables:

- `realtime` - Real-time sensor data
- `alert` - Alert information

## ğŸš€ Docker Compose Testing

For complete infrastructure testing, see `DOCKER_COMPOSE_TEST.md`:

```bash
# Start entire infrastructure
docker-compose up -d

# Test DataLake API service
curl http://localhost:8084/healthz
curl http://localhost:8084/stats

# Test DataLake dashboard
# Open http://localhost:8501 in your browser
```

## ğŸš€ Azure App Container Service ë°°í¬

Azure App Container Serviceì˜ í¬íŠ¸ ì œì•½ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ APIì™€ ëŒ€ì‹œë³´ë“œë¥¼ ë¶„ë¦¬í•˜ì—¬ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### API ì„œë¹„ìŠ¤ ë°°í¬

```bash
chmod +x deploy-api.sh
./deploy-api.sh <resource-group> <app-name>
```

### ëŒ€ì‹œë³´ë“œ ì„œë¹„ìŠ¤ ë°°í¬

```bash
chmod +x deploy-dashboard.sh
./deploy-dashboard.sh <resource-group> <app-name>
```

## ğŸ“Š Monitoring

- **Real-time records count**
- **Active alerts count**
- **Storage type and configuration**
- **Batch scheduler status**
- **Storage statistics (MockStorage)**
- **Uploaded batches tracking**

## ğŸ”„ Data Processing Flow

### 1. Real-time Data Ingestion

```bash
# Mock Serverì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì‹  ë° Redis ì €ì¥
curl -X POST http://localhost:8080/ingest

# ì™¸ë¶€ APIì—ì„œ ë°ì´í„° ìˆ˜ì‹  ë° Redis ì €ì¥
curl -X POST http://localhost:8080/ingest/external \
  -H "Content-Type: application/json" \
  -d '{
    "equipment_id": "sensor_001",
    "facility_id": "facility_001",
    "measured_at": "2024-01-01T12:00:00Z",
    "temperature": 25.5,
    "humidity": 60.0
  }'
```

### 2. Redis Data Management

```bash
# Redis ìƒíƒœ ë° ì„¼ì„œ ë°ì´í„° ê°œìˆ˜ í™•ì¸
curl http://localhost:8080/redis/status

# ê°•ì œ Redis flush ì‹¤í–‰
curl -X POST http://localhost:8080/trigger-redis-flush
```

### 3. Monitoring & Statistics

```bash
# ì„œë¹„ìŠ¤ í†µê³„ í™•ì¸ (Redis ë°ì´í„° ê°œìˆ˜ í¬í•¨)
curl http://localhost:8080/stats

# ë°°ì¹˜ ì—…ë¡œë“œ ê°•ì œ ì‹¤í–‰
curl -X POST http://localhost:8080/trigger-batch-upload
```

## ğŸ“¡ Event Publishing Rules

### Kafka Event Types

#### 1. sensorDataAnomalyDetected

- **When**: ì´ìƒì¹˜ íƒì§€ ì‹œ ì¦‰ì‹œ ë°œí–‰
- **Topic**: `datalake.sensorDataAnomalyDetected`
- **Usage**: ì‹¤ì‹œê°„ ì•Œë¦¼ ë° ê²½ê³  ì‹œìŠ¤í…œ

#### 2. sensorDataSaved âš ï¸ IMPORTANT

- **When**: Redis flush ì‹œì—ë§Œ ë°œí–‰ (1ë¶„ ê°„ê²©)
- **Topic**: `datalake.sensorDataSaved`
- **Usage**: ë°ì´í„° ì˜ì†ì„± í™•ì¸ ë° ë°°ì¹˜ ì²˜ë¦¬
- **NOT for**: ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì‹  ì‹œ

#### 3. Legacy sensorData (Deprecated)

- **When**: ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
- **Topic**: `datalake.sensorData`
- **Note**: ì´ ë©”ì„œë“œëŠ” Redis flush ì‹œì—ë§Œ ì‚¬ìš©ë˜ì–´ì•¼ í•¨

### Event Flow Diagram

```
Real-time Data â†’ Redis Storage â†’ Periodic Flush (1min) â†’ Local Storage â†’ sensorDataSaved Event
     â†“
Anomaly Detection â†’ sensorDataAnomalyDetected Event (Immediate)
```

### Common Mistakes to Avoid

âŒ **Don't**: Call `publish_data_saved` during real-time data ingestion
âœ… **Do**: Call `publish_data_saved` only during Redis flush operations
âŒ **Don't**: Use `publish_sensor_data` for new implementations
âœ… **Do**: Use `publish_data_saved` for data persistence events
